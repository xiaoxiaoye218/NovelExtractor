# NovelExtractor｜小说提取工具

一个小说拆书工具，利用AI对超长的小说内容进行任意的拆解，比如"剧情压缩"，"世界地图归纳"，"战斗描述拆解"，"指定相似剧情查找"等。

本工具旨在利用AI进行拟人化的阅读，辅助作者进行拆书，缩短作者从一本书中获得自己需要的内容的时间。

## 目录结构

```text
NovelExtractor/
├─ run_ui.py                    # 启动图形界面
├─ requirements.txt             # 依赖
├─ configs/
│  └─ config.json               # 模型配置
├─ app/
│  ├─ query.py                  # 批量 LLM 查询逻辑（并发/批次/断点重续）
│  ├─ novel_pre_processor.py    # 预处理，将整本小说逐章节拆分并保存为txt
│  └─ merge_files.py            # txt文件合并工具
├─ pyqt_ui/
│  ├─ main_window.py            # 主窗体（多标签页）
│  ├─ query_ui.py               # 查询页（利用AI对小说内容进行处理）
│  ├─ merge_files_ui.py         # 合并页
│  ├─ novel_pre_processor_ui.py # 预处理页
│  ├─ reader_ui.py              # 阅读页（对AI生成的内容进行查看）
│  └─ config_ui.py              # 配置页
└─ utils/
   ├─ unified_chat.py           # 多厂商模型统一路由
   ├─ text_processor.py         # 将txt小说逐章节拆分提取到变量中
   ├─ readtxt.py                # 自动编码识别读取
   └─ paths.py                  # 路径定位
```

## 厂商兼容

- 目前仅实现了OpenAI兼容格式
- Gemini 待实现
- Anthropic 待实现（有没有好心的佬投喂一下apikey支持）

## 使用流程

1. 先在"配置"页设置 `api_key`、`base_url`，添加可用 `models`

2. 在"小说预处理"页：选择整本小说 `.txt`，输出到一个章节目录（自动识别"第X章"并按自然顺序拆分）

   一定要先做，因为query查询页面以一个txt为最小输入单位，内部不会再自动拆分成多个章节

3. 在"查询"页：
   - 选择"输入目录"为步骤 2 生成的章节目录（或者此前的查询结果作为输入，只要是多个命名格式相同的txt都可以）
   - 选择"输出目录"与 Prompt 文件
   - 选择 Provider/Model，设置并发与批次大小，开始查询
   - 结果会生成如 `查询结果_bs30_批次1.txt`、`查询结果_bs30_批次2.txt` …

4. "小说阅读"页：打开章节目录，会加载进行浏览

## 小说预处理页面使用说明

输入要拆书的文件路径，例如：
```
C:\Users\acer\Desktop\NovelExtractor\example\我欲封天\wyft.txt
```

输出逐章拆分的章节内容，例如：
```
📁 我欲封天_章节拆分
           ├── 📄 第1章_书生孟浩_1.txt        (11 KB, 2025/9/28 20:33)
           ├── 📄 第2章_靠山宗_2.txt          (10 KB, 2025/9/28 20:33)
           ├── 📄 第3章_晋升外宗_3.txt        (15 KB, 2025/9/28 20:33)
           ├── 📄 第4章_一面铜镜_4.txt        (11 KB, 2025/9/28 20:33)
           ├── 📄 第5章_此子不凡_5.txt        (11 KB, 2025/9/28 20:33)
           ├── 📄 第6章_铜镜的快乐_6.txt      (10 KB, 2025/9/28 20:33)
```

## Query页面说明

示例如下：

```
┌──────────────────────────────────────────────────────────────────────────────┐
│                            小说提取工具                                     │
├──────────────────────────────────────────────────────────────────────────────┤
│  [小说预处理]  [合并文件]  [查询]  [小说阅读]  [配置]                       │
├──────────────────────────────────────────────────────────────────────────────┤
│ 输入目录: C:/Users/acer/Desktop/NovelExtractor/example/我欲封天/章节拆分     │
│                  [选择目录]                                                │
├──────────────────────────────────────────────────────────────────────────────┤
│ 输出目录: C:/Users/acer/Desktop/NovelExtractor/example/我欲封天/相似剧情查询 │
│                  [选择目录]                                                │
├──────────────────────────────────────────────────────────────────────────────┤
│ 提供商:          [doubao ▼]                                                 │
│ 模型ID:          [doubao-seed-1-6-flash-250828 ▼]                           │
│ 并发数量:        [30 ▲▼]                                                    │
│ 批次大小:        [9 ▲▼]                                                     │
├──────────────────────────────────────────────────────────────────────────────┤
│ Prompt文件: C:/Users/acer/Desktop/NovelExtractor/example/我欲封天/prompts指令/相似剧情寻找.txt │
│                  [选择文件]                                                │
├──────────────────────────────────────────────────────────────────────────────┤
│ 输出文件名前缀: [查询结果]                                                 │
├──────────────────────────────────────────────────────────────────────────────┤
│ 起始位置（可选）: [0 ▲▼]                                                   │
│ 终止位置（可选）: [0 ▲▼]                                                   │
├──────────────────────────────────────────────────────────────────────────────┤
│              [开始查询]                             [中止]                  │
└──────────────────────────────────────────────────────────────────────────────┘

┌──────────────────────────────────────────────────────────────────────────────┐
│ 开始查询...                                                                 │
│ 总共找到 204 个 txt 文件                                                    │
│ 需要处理 23 个批次                                                          │
│ 开始并发处理 23 个批次（并发数：30）
```

### Prompts示例

```
帮我根据下面的内容，寻找是否存在与我想要写的剧情类似的情节，如果存在请输出，不存在则输出"不存在"
我构思的剧情：主角面临困境，有多种选择，但是主角没有固守成规，选择某一种方法，而是选择了常人不会想到的方法，从而破局
注意，要明确存在有"选择"，主角要明确需要做选择题，不能把不相似的剧情放进来

----------------------------------------------------------------------------
{input_content}
----------------------------------------------------------------------------

要求如下：
帮我根据上面的内容，寻找是否存在与我想要写的剧情类似的情节，如果不存在则输出"不存在"
我构思的剧情：主角面临有多种选择，比如主角被给予了A or B的选择，主角选择了C或者or的感觉，主角没有固守成规，选择某一种方法，而是选择了常人不会想到的方法，从而破局
注意，要明确存在有"选择"，主角要明确需要做选择题，不能把不相似的剧情放进来
如果存在，请输出这部分剧情的概要，然后换行输出这部分剧情的章节原文
```

### 参数说明

- `input_path`：输入参数的文件路径
- `output_path`：输出参数的文件路径
- `provider/model`：大模型配置
- `concurrent`：异步并发数量（理论上越高越好，但是考虑到厂商RPM和TPM限制）
- `batch_size`：每一批输入的章节数/文件数（建议输入为原文章节时设置为9以内，因为有些模型厂商阶梯计价，9章原文字数可以确保控制在32k之下）
- `prompt_path`：你要让LLM每次调用时要传入的指令，要有{input_content}占位符
- `文件前缀`：每个批次会有一个输出文件，这个对应每次输出的文件的文件名前缀

### 链式执行说明

注意到这个query.py是可以多次链式执行的，就是可能涉及到的章节内容太长了，那么我们可以分成多个批次分别处理，然后根据第一次query处理的结果再继续作为输入调用query.py

例如：
1. 首先逐章节压缩原文，得到原文的压缩版本
2. 然后对原文的压缩版本进行查询任务，比如相似剧情查找等，可以节约token，也可以让AI一次性看到更多的内容；缺点就是压缩是一定会丢失故事细节的

## 小说阅读页面使用说明

- 可以直接键盘上下键快速切换文件预览
- 可以编辑文件内容

## 注意事项

查询页面的输入文件目录，只能有一个格式的txt文件，比如"小说预处理"页面的输出目录，不能把多个小说拆分的txt输出到同一个目录下；又比如query查询页面的输出，每次要单独指定一个目录，不然这个目录就没法作为下一次query的输入目录了。不然可能会报错类似于："查询过程中发生错误: '<' not supported between instances of 'int' and 'tuple'"

一本小说的各种拆分内容的管理只能依靠于使用者自己对于每次query的输出目录的命名管理。

## 不使用RAG做向量相似度匹配的原因

RAG只能做相似度匹配，它其实并不适合做知识库。

"搜索思想之情 得到 床前明月光"这样的操作通过RAG是做不到的，搜索"装逼打脸"，得到的最相似的片段反而是真的在文本里有"打脸"这两个字的，而不是抽象的"装逼"行为。

所以老实说，做知识库，其实得用LLM来做搜索，而不是向量数据库；LLM能理解一段剧情文本是装逼打脸，但是embedding模型理解不了，有可能是参数规模太少，也有可能是因为训练方式的差异。

我对qwen3系列的4B左右小模型测试了一下，理解测试偶尔成功也偶尔掉线，这说明参数是影响很大的。

